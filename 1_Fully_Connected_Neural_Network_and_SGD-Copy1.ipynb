{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3brsgM_WOZd"
   },
   "source": [
    "## In this notebook, you will implement a two-layer neural network model and stochastic gradient descent with momentum in numpy to classify CIFAR-10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMyjktItWOZg",
    "outputId": "8d45a356-67b6-4da6-a403-aaf96f0fc02c"
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# load CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).astype('float')\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype('float')\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "x_train = x_train - x_train.mean(axis=1, keepdims=True)\n",
    "x_train = x_train / x_train.std(axis=1, keepdims=True)\n",
    "x_test = x_test - x_test.mean(axis=1, keepdims=True)\n",
    "x_test = x_test / x_test.std(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDDJ4YNpSVkP"
   },
   "source": [
    "# Implement the forward and backward function for a two-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "VQal1IREWOZh"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNet():\n",
    "  def __init__(self, hidden_dim, input_dim=3072, num_classes=10):\n",
    "    super(FullyConnectedNet, self).__init__()\n",
    "    self.params = {}\n",
    "    self.params['W1'] = np.random.randn(input_dim, hidden_dim)/np.sqrt(input_dim/2)\n",
    "    self.params['b1'] = np.zeros(hidden_dim)\n",
    "    self.params['W2'] = np.random.randn(hidden_dim, num_classes)/np.sqrt(hidden_dim/2)\n",
    "    self.params['b2'] = np.zeros(num_classes)\n",
    "    self.grads = {}\n",
    "    self.grads['W1'] = np.zeros_like(self.params['W1'])\n",
    "    self.grads['b1'] = np.zeros_like(self.params['b1'])\n",
    "    self.grads['W2'] = np.zeros_like(self.params['W2'])\n",
    "    self.grads['b2'] = np.zeros_like(self.params['b2'])\n",
    "        \n",
    "  def forward(self, x):\n",
    "    \"\"\"Forward pass: linear -- ReLU -- linear\n",
    "    \n",
    "    Arguments:\n",
    "        x: 2D numpy array of shape (batch_size, input_dim)\n",
    "        \n",
    "    Returns:\n",
    "        scores: 2D numpy array of shape (batch_size, num_classes)\n",
    "        cache: a tuple of numpy arrays, used for backward pass\n",
    "    \"\"\"\n",
    "############################Write your code in this block (20 points)#####################\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = x.shape \n",
    "    \n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "  \n",
    "    Z1 = x.dot(W1) + b1\n",
    "    A1 = np.maximum(0, Z1)\n",
    "\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    A2 = Z2\n",
    "    scores = A2\n",
    "        \n",
    "    # Compute the loss\n",
    "    cache = None\n",
    "    scores -= np.max(scores, axis=1,keepdims=True)\n",
    "      \n",
    "    scores_e = np.exp(scores)\n",
    "\n",
    "    cache = np.sum(-1  + np.log(np.sum(scores_e))) / N\n",
    "    cache += 0.5 *  0.0 *(np.sum(W1 * W1) + np.sum(W2 * W2))   \n",
    "    self.grads['x']=x\n",
    "   \n",
    "    #################################End of your code#########################################\n",
    "    return scores, cache\n",
    "    \n",
    "  def backward(self, dscores, cache):\n",
    "    \"\"\"Backward pass\n",
    "    \n",
    "    Arguments:\n",
    "        dscores: 2D numpy array of shape (batch_size, num_classes)\n",
    "        cache: a tuple of numpy arrays saved from forward pass\n",
    "        \n",
    "    Returns:\n",
    "        no explicit variable returned\n",
    "        update self.grads['W1'], self.grads['b1'], self.grads['W2'], self.grads['b2']\n",
    "    \"\"\"\n",
    "    #############################Write your code in this block (30 points)######################\n",
    "    \n",
    "    x=self.grads['x']\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = x.shape \n",
    "   \n",
    "    Z1 = x.dot(W1) + b1\n",
    "    A1 = np.maximum(0, Z1)\n",
    "\n",
    "    B = np.full((N, 1), 1.0)\n",
    "\n",
    "    dL =dscores\n",
    "    self.grads['W2'] = A1.T.dot(dL) + 0.0 * W2\n",
    "    self.grads['b2'] = B.T.dot(dL)\n",
    "\n",
    "    dA1 = dL.dot(W2.T) * (Z1 > 0)\n",
    "    self.grads['W1'] = x.T.dot(dA1) + 0.0 *  W1\n",
    "    self.grads['b1'] = B.T.dot(dA1)\n",
    "    ##################################End of your code##########################################\n",
    "    return cache, self.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "Z4SZwmhDUAm4"
   },
   "outputs": [],
   "source": [
    "# The Cross Entropy Loss has been implemented for you\n",
    "def cross_entropy_loss(scores, y, eps=1e-8):\n",
    "  \"\"\"Calculate cross entropy loss and its gradient with respect to scores\n",
    "  \n",
    "  Arguments:\n",
    "      scores: 2D numpy array of shape (batch_size, num_classes), predicted class scores\n",
    "      y: 1D numpy array of shape (batch_size,), true class labels (from 0 to num_classes-1)\n",
    "      \n",
    "  Returns:\n",
    "      loss: scalar\n",
    "      dscores: 2D numpy array of shape (batch_size, num_classes), loss's gradient with respect to scores\n",
    "      \n",
    "  \"\"\"\n",
    "  scores -= scores.max()\n",
    "  exp = np.exp(scores)\n",
    "  exp = np.maximum(exp, eps)\n",
    "  logits = - scores[range(scores.shape[0]), y] + np.log(exp.sum(axis=1))\n",
    "  loss = logits.mean()\n",
    "  dscores = np.zeros_like(scores)\n",
    "  dscores[range(scores.shape[0]), y] = -1\n",
    "  dscores += exp/exp.sum(axis=1, keepdims=True)\n",
    "  dscores /= scores.shape[0]\n",
    "  return loss, dscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4f0QQaqUPYD"
   },
   "source": [
    "# Implement SGD with momentum\n",
    "Initialize $m^{(0)}=0$\n",
    "\n",
    "After $k$ steps, update $m^{(k+1)}$ and $\\theta^{(k+1)}$:\n",
    "$$m^{(k+1)}=\\beta \\cdot m^{(k)} + \\nabla_{\\theta} $$\n",
    "$$\\theta^{(k+1)}=\\theta^{(k)} - \\alpha \\cdot m^{(k+1)}$$\n",
    "If you want to have a deep understanding of SGD with momentum, I recommend [this article](https://distill.pub/2017/momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "nS5BZP87UIco"
   },
   "outputs": [],
   "source": [
    "class SGD():\n",
    "  def __init__(self, params, alpha, beta=0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      params: a dictionary, with keys being variable names, and values numpy arrays\n",
    "      alpha: learning rate as shown in the formula in above text cell\n",
    "      beta: the momentum coefficient, as shown in the formula in the above text cell;\n",
    "        if beta = 0, then it becomes vallina SGD; Can you figure out why?\n",
    "    \"\"\"    \n",
    "    self.params = params\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    # Initially, set the momentum m = 0 for all parameters\n",
    "    self.momentum = {n: np.zeros_like(p) for n, p in self.params.items()}\n",
    "\n",
    "  def step(self, grads):\n",
    "    \"\"\"Perform gradient descent\n",
    "    Arguments:\n",
    "        grads: a dictionary, with the same keys as self.params, storing the corresponding gradients;\n",
    "               for example, grads['W1'] is the gradient with respect to self.params['W1']\n",
    "    Returns:\n",
    "        No explicit returns\n",
    "        Update self.params internally\n",
    "    \"\"\"\n",
    "    #Hint: self.params is a dictionary of parameters,\n",
    "    ###### use a for loop to iterate all the items from self.params,\n",
    "    ###### use the formula in the above text cell to update each parameter;\n",
    "    ###### Note self.momentum and grads are all dictionaries with the same keys as self.params\n",
    "    ##############################Write your code in this block (20 points)######################\n",
    "    #https://datascience-enthusiast.com/DL/Optimization_methods.html\n",
    "    #https://blog.paperspace.com/optimization-in-deep-learning/\n",
    "    for l in range(len(self.params)):\n",
    "        self.momentum[str(l+1)] = self.beta * self.momentum[str(l+1)] + self.alpha * grads[str(l+1)]\n",
    "        theta[i] = self.params[i] - self.momentum[i]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    ###################################End of your code##########################################\n",
    "    return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4b07yQhWOZi"
   },
   "source": [
    "# Test your implementation using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "WlYsyi7nWOZi"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "model = FullyConnectedNet(hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4h5pDB-WOZj",
    "outputId": "a0f8cec1-42aa-414a-8bd6-57be4eeae128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, training accuracy=0.11996, test accuracy=0.1243\n"
     ]
    }
   ],
   "source": [
    "y_pred, _ = model.forward(x_train)\n",
    "acc_train = np.mean(y_pred.argmax(axis=1) == y_train)\n",
    "y_pred, _ = model.forward(x_test)\n",
    "acc_test = np.mean(y_pred.argmax(axis=1) == y_test)\n",
    "print(f'Before training, training accuracy={acc_train}, test accuracy={acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "7DPL2x5oWOZj"
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "acc_train_history = []\n",
    "acc_val_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvEm7nsyWOZk",
    "outputId": "f200806e-b8d0-479d-84b1-f3b67fd16ace"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PRANAL~1\\AppData\\Local\\Temp/ipykernel_4912/910130034.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PRANAL~1\\AppData\\Local\\Temp/ipykernel_4912/3436241021.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "num_iters = 2000\n",
    "batch_size = 40\n",
    "alpha = 1e-2\n",
    "beta = 1\n",
    "optimizer = SGD(model.params, alpha=alpha, beta=beta)\n",
    "print_every = num_iters//20\n",
    "for i in range(num_iters):\n",
    "    idx = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "    scores, cache = model.forward(x_batch)\n",
    "    loss, dscores = cross_entropy_loss(scores, y_batch)\n",
    "    model.backward(dscores, cache)\n",
    "    optimizer.step(model.grads)\n",
    "    loss_history.append(loss.item())\n",
    "    acc_train = np.mean(scores.argmax(axis=1) == y_batch)\n",
    "    acc_train_history.append(acc_train.item())\n",
    "    # test accuracy\n",
    "    idx = np.random.choice(x_test.shape[0], batch_size)\n",
    "    x_batch = x_test[idx]\n",
    "    y_batch = y_test[idx]\n",
    "    scores, cache = model.forward(x_batch)\n",
    "    acc_val = np.mean(scores.argmax(axis=1) == y_batch)\n",
    "    acc_val_history.append(acc_val.item())\n",
    "    if i == 0 or i == num_iters-1 or (i+1)%print_every == 0:\n",
    "        print(f'{i+1} loss={loss}, acc_train={acc_train}, acc_val={acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "_Q2yzUzDWOZk",
    "outputId": "02dddd52-482d-4c67-c9dc-8ab631275f18"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.plot(acc_train_history, 'b-', label='train')\n",
    "plt.plot(acc_val_history, 'g-', label='val')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7EV431gWOZl",
    "outputId": "78295cbf-a3a7-4e43-c9a5-9552ba9daf7e"
   },
   "outputs": [],
   "source": [
    "y_pred, _ = model.forward(x_train)\n",
    "acc_train = np.mean(y_pred.argmax(axis=1) == y_train)\n",
    "y_pred, _ = model.forward(x_test)\n",
    "acc_test = np.mean(y_pred.argmax(axis=1) == y_test)\n",
    "## You should achieve a test accuracy at least 0.4\n",
    "print(f'After training, training accuracy={acc_train}, test accuracy={acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMzSMdZzWOZl"
   },
   "source": [
    "# Try different hyperparameters: hidden_dim, lr, batch_size, num_iters, find one that can achieve the best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "id": "rFJ8iKnqWOZl",
    "outputId": "d8ab4cb4-9072-4aba-cb59-547acdc65b36"
   },
   "outputs": [],
   "source": [
    "###############Modify these hyperparameters and find the best ones (20 points)##################\n",
    "###############You can write a loop to search for the best parameters if your want##############\n",
    "hidden_dim = 1000\n",
    "alpha = 1e-2\n",
    "beta = 10\n",
    "batch_size = 40\n",
    "num_iters = 2000\n",
    "#################################################################################################\n",
    "\n",
    "model = FullyConnectedNet(hidden_dim=hidden_dim)\n",
    "optimizer = SGD(model.params, alpha=alpha, beta=beta)\n",
    "loss_history = []\n",
    "acc_train_history = []\n",
    "acc_val_history = []\n",
    "print_every = num_iters//20\n",
    "for i in range(num_iters):\n",
    "    idx = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "    scores, cache = model.forward(x_batch)\n",
    "    loss, dscores = cross_entropy_loss(scores, y_batch)\n",
    "    model.backward(dscores, cache)\n",
    "    optimizer.step(model.grads)\n",
    "    loss_history.append(loss.item())\n",
    "    acc_train = np.mean(scores.argmax(axis=1) == y_batch)\n",
    "    acc_train_history.append(acc_train.item())\n",
    "    # test accuracy\n",
    "    idx = np.random.choice(x_test.shape[0], batch_size)\n",
    "    x_batch = x_test[idx]\n",
    "    y_batch = y_test[idx]\n",
    "    scores, cache = model.forward(x_batch)\n",
    "    acc_val = np.mean(scores.argmax(axis=1) == y_batch)\n",
    "    acc_val_history.append(acc_val.item())\n",
    "    if i == 0 or i == num_iters-1 or (i+1)%print_every == 0:\n",
    "        print(f'{i+1} loss={loss}, acc_train={acc_train}, acc_val={acc_val}')\n",
    "        \n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "plt.plot(acc_train_history, 'b-', label='train')\n",
    "plt.plot(acc_val_history, 'g-', label='val')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred, _ = model.forward(x_train)\n",
    "acc_train = np.mean(y_pred.argmax(axis=1) == y_train)\n",
    "y_pred, _ = model.forward(x_test)\n",
    "acc_test = np.mean(y_pred.argmax(axis=1) == y_test)\n",
    "## You should achieve a test accuracy at least 0.4\n",
    "print(f'After training, training accuracy={acc_train}, test accuracy={acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
